{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Embedding Projector in TensorBoard\n",
    "\n",
    "Based on the tensorboard tutorial in this [link](https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin).\n",
    "\n",
    "Data can be found [here](https://www.kaggle.com/akudnaver/amazon-reviews-dataset).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-0rhuaW9f2-"
   },
   "source": [
    "## Setup\n",
    "\n",
    "For this tutorial, we will be using TensorBoard to visualize an embedding layer generated for classifying amazon review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TjRkD3r3etuL"
   },
   "outputs": [],
   "source": [
    "'''try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass'''\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# NN libraries and embeddings\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins import projector\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import utils as skl_utils\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Text preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim import utils\n",
    "import gensim.parsing.preprocessing as gsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "1.3.4\n",
      "3.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.8.0\\n1.3.4\\n3.6.7\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(pd.__version__)\n",
    "print(nltk.__version__)\n",
    "'''2.8.0\n",
    "1.3.4\n",
    "3.6.7\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('review-details.xlsx', engine = 'openpyxl', usecols= ['review_title', 'review_text', 'review_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>5</td>\n",
       "      <td>Great savings</td>\n",
       "      <td>Love this smell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>5</td>\n",
       "      <td>Does what its made for</td>\n",
       "      <td>Brilliant product! Left the drains clear and clean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>I will be buying again thank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      review_rating            review_title  \\\n",
       "2008              5           Great savings   \n",
       "315               5  Does what its made for   \n",
       "170               5                    None   \n",
       "\n",
       "                                             review_text  \n",
       "2008                                     Love this smell  \n",
       "315   Brilliant product! Left the drains clear and clean  \n",
       "170                         I will be buying again thank  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "    remove_words = stopwords.words('english')\n",
    "    remove_words.remove('not')\n",
    "    remove_words.remove('no')\n",
    "    STOPWORDS = set(remove_words)\n",
    "    text = text.lower()  # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub(' ', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    #text = ' '.join(word for word in text.split() if word not in remove_pn)\n",
    "\n",
    "    return text\n",
    "\n",
    "contraction_dict = {\"ain’t\": \"is not\", \"aren’t\": \"are not\",\"can’t\": \"cannot\", \"’cause\": \"because\",\n",
    "                    \"could’ve\": \"could have\", \"couldn’t\": \"could not\", \"didn’t\": \"did not\",\n",
    "                    \"doesn’t\": \"does not\", \"don’t\": \"do not\", \"hadn’t\": \"had not\", \"hasn’t\": \"has not\",\n",
    "                    \"haven’t\": \"have not\", \"he’d\": \"he would\",\"he’ll\": \"he will\", \"he’s\": \"he is\",\n",
    "                    \"how’d\": \"how did\", \"how’d’y\": \"how do you\", \"how’ll\": \"how will\", \"how’s\": \"how is\",\n",
    "                    \"I’d\": \"I would\", \"I’d’ve\": \"I would have\", \"I’ll\": \"I will\", \"I’ll’ve\": \"I will have\",\n",
    "                    \"I’m\": \"I am\", \"I’ve\": \"I have\", \"i’d\": \"i would\", \"i’d've\": \"i would have\",\n",
    "                    \"i’ll\": \"i will\",  \"i’ll’ve\": \"i will have\",\"i’m\": \"i am\", \"i’ve\": \"i have\",\n",
    "                    \"isn’t\": \"is not\", \"it’d\": \"it would\", \"it’d’ve\": \"it would have\", \"it’ll\": \"it will\",\n",
    "                    \"it’ll've\": \"it will have\",\"it’s\": \"it is\", \"let’s\": \"let us\", \"ma’am\": \"madam\",\n",
    "                    \"mayn’t\": \"may not\", \"might’ve\": \"might have\",\"mightn’t\": \"might not\",\n",
    "                    \"mightn’t’ve\": \"might not have\", \"must’ve\": \"must have\", \"mustn’t\": \"must not\",\n",
    "                    \"mustn’t’ve\": \"must not have\", \"needn’t\": \"need not\", \"needn’t’ve\": \"need not have\",\n",
    "                    \"o’clock\": \"of the clock\", \"oughtn’t\": \"ought not\", \"oughtn’t’ve\": \"ought not have\",\n",
    "                    \"shan’t\": \"shall not\", \"sha’n’t\": \"shall not\", \"shan’t’ve\": \"shall not have\",\n",
    "                    \"she’d\": \"she would\", \"she’d’ve\": \"she would have\", \"she’ll\": \"she will\",\n",
    "                    \"she’ll’ve\": \"she will have\", \"she’s\": \"she is\", \"should’ve\": \"should have\",\n",
    "                    \"shouldn’t\": \"should not\", \"shouldn’t’ve\": \"should not have\", \"so’ve\": \"so have\",\n",
    "                    \"so’s\": \"so as\", \"this’s\": \"this is\",\"that’d\": \"that would\", \"that’d’ve\": \"that would have\",\n",
    "                    \"that’s\": \"that is\", \"there’d\": \"there would\", \"there’d’ve\": \"there would have\",\n",
    "                    \"there’s\": \"there is\", \"here’s\": \"here is\",\"they’d\": \"they would\",\n",
    "                    \"they’d’ve\": \"they would have\", \"they’ll\": \"they will\", \"they’ll’ve\": \"they will have\",\n",
    "                    \"they’re\": \"they are\", \"they’ve\": \"they have\", \"to’ve\": \"to have\", \"wasn’t\": \"was not\",\n",
    "                    \"we’d\": \"we would\", \"we’d’ve\": \"we would have\", \"we’ll\": \"we will\", \"we’ll’ve\": \"we will have\",\n",
    "                    \"we’re\": \"we are\", \"we’ve\": \"we have\", \"weren’t\": \"were not\", \"what’ll\": \"what will\",\n",
    "                    \"what’ll’ve\": \"what will have\", \"what’re\": \"what are\",  \"what’s\": \"what is\",\n",
    "                    \"what’ve\": \"what have\", \"when’s\": \"when is\", \"when’ve\": \"when have\", \"where’d\": \"where did\",\n",
    "                    \"where’s\": \"where is\", \"where’ve\": \"where have\", \"who’ll\": \"who will\",\n",
    "                    \"who’ll’ve\": \"who will have\", \"who’s\": \"who is\", \"who’ve\": \"who have\", \"why’s\": \"why is\",\n",
    "                    \"why’ve\": \"why have\", \"will’ve\": \"will have\", \"won’t\": \"will not\", \"won’t’ve\": \"will not have\",\n",
    "                    \"would’ve\": \"would have\", \"wouldn’t\": \"would not\", \"wouldn’t’ve\": \"would not have\",\n",
    "                    \"y’all\": \"you all\", \"y’all’d\": \"you all would\",\"y’all’d’ve\": \"you all would have\",\n",
    "                    \"y’all’re\": \"you all are\",\"y’all’ve\": \"you all have\",\"you’d\": \"you would\",\n",
    "                    \"you’d’ve\": \"you would have\", \"you’ll\": \"you will\", \"you’ll’ve\": \"you will have\",\n",
    "                    \"you’re\": \"you are\", \"you’ve\": \"you have\"}\n",
    "\n",
    "def _get_contractions(contraction_dict):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_dict, contraction_re\n",
    "\n",
    "contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "filters_key = [\n",
    "        gsp.strip_tags, \n",
    "        gsp.strip_punctuation,\n",
    "        gsp.strip_multiple_whitespaces,\n",
    "        gsp.strip_numeric,\n",
    "        #gsp.remove_stopwords, \n",
    "        #gsp.strip_short, \n",
    "        #gsp.stem_text\n",
    "        ]\n",
    "\n",
    "def clean_gsm_key(s):\n",
    "    #s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters_key:\n",
    "        s = f(s)\n",
    "    return s\n",
    "\n",
    "filters = [\n",
    "        gsp.strip_tags, \n",
    "        gsp.strip_punctuation,\n",
    "        gsp.strip_multiple_whitespaces,\n",
    "        gsp.strip_numeric,\n",
    "        #gsp.remove_stopwords, \n",
    "        gsp.strip_short, \n",
    "        #gsp.stem_text\n",
    "        ]\n",
    "\n",
    "def clean_gsm(s):\n",
    "    #s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s\n",
    "\n",
    "def contrac_lemm(df, comments):\n",
    "    '''\n",
    "    Trying to lemmatize and clean.\n",
    "    Assumes text is lower case.\n",
    "    '''\n",
    "    #import contractions\n",
    "    import re\n",
    "    import string\n",
    "    import nltk\n",
    "    #nltk.download('wordnet')\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    def lemmatize_text(text):\n",
    "        return [lemmatizer.lemmatize(w, \"v\") for w in w_tokenizer.tokenize(text)]\n",
    "    #df['no_contract'] = df[comments_column].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "    df['list_lemmatized'] = df[comments].apply(lemmatize_text)\n",
    "    df[comments] = [' '.join(map(str, l)) for l in df['list_lemmatized']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_text</th>\n",
       "      <th>clean_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Dove Men’s + Deodorant</td>\n",
       "      <td>As you get older, you know what you like and what is suitable for your body. I like all Dove products. Gives you that fresh all over, wide awake feeling and no dandruff or flakey skin. No smelly a/pits!</td>\n",
       "      <td>get older know like suitable body like dove products gives fresh wide awake feeling no dandruff flakey skin no smelly pits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Great for a marmite lover!</td>\n",
       "      <td>Three gigantic marmite jars that will last probably a whole life! What else would you possibly wish for? Order came in time, when mentioned, safely packed. Very happy with it.</td>\n",
       "      <td>three gigantic marmite jars last probably whole life else would possibly wish order came time mentioned safely packed happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_rating                review_title  \\\n",
       "0              5      Dove Men’s + Deodorant   \n",
       "1              5  Great for a marmite lover!   \n",
       "\n",
       "                                                                                                                                                                                                  review_text  \\\n",
       "0  As you get older, you know what you like and what is suitable for your body. I like all Dove products. Gives you that fresh all over, wide awake feeling and no dandruff or flakey skin. No smelly a/pits!   \n",
       "1                             Three gigantic marmite jars that will last probably a whole life! What else would you possibly wish for? Order came in time, when mentioned, safely packed. Very happy with it.   \n",
       "\n",
       "                                                                                                                clean_comments  \n",
       "0   get older know like suitable body like dove products gives fresh wide awake feeling no dandruff flakey skin no smelly pits  \n",
       "1  three gigantic marmite jars last probably whole life else would possibly wish order came time mentioned safely packed happy  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anieto/miniconda3/envs/zta/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "df[\"clean_comments\"] = df[\"review_text\"].apply(lambda x: str(x).lower())\n",
    "df['clean_comments'] = df['clean_comments'].str.replace('\\'', '’')\n",
    "df['clean_comments'] = df['clean_comments'].str.replace('´', '’')\n",
    "df['clean_comments'] = df['clean_comments'].str.replace('‚Äô', '’')\n",
    "df['clean_comments'] = df['clean_comments'].str.replace('‚äô', '’')\n",
    "df['clean_comments'] = df['clean_comments'].str.replace('â€™', '’')\n",
    "df[\"clean_comments\"] = df[\"clean_comments\"].apply(lambda x: remove_URL(x))\n",
    "df['clean_comments'] = df['clean_comments'].apply(lambda x: replace_contractions(x))\n",
    "df['clean_comments'] = df['clean_comments'].apply(lambda x: clean_text(x))\n",
    "df['clean_comments'] = df['clean_comments'].str.replace('\\d+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_rating      0\n",
       "review_title      98\n",
       "review_text        0\n",
       "clean_comments     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('review_rating', axis=1)\n",
    "messages = X.copy()\n",
    "messages = messages.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(0, len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['clean_comments'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exactli say kitchen sink alway get block sinc put block last month'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LE = LabelEncoder()\n",
    "df['relevance_enc'] = LE.fit_transform(df['review_rating'])\n",
    "X = df.clean_comments #the column text contains textual data to extract features from\n",
    "y = df.relevance_enc #this is the column we are learning to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to produce a dictionary to retrieve correct tags\n",
    "text_tags = df['relevance_enc'].unique()\n",
    "text_tags = list(np.sort(text_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_test, y_tr, y_test = train_test_split(X, y,\n",
    "    train_size=0.9,\n",
    "    test_size=0.1,\n",
    "    # random but same for all run\n",
    "    random_state=2022,\n",
    "    # keep same proportion of 'target' in test and target data\n",
    "    stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr,\n",
    "    train_size=0.8,\n",
    "    test_size=0.2,\n",
    "    # random but same for all run\n",
    "    random_state=2022,\n",
    "    # keep same proportion of 'target' in test and target data\n",
    "    stratify=y_tr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = X_train\n",
    "train_labels = y_train\n",
    "test_texts = X_test\n",
    "test_labels = y_test\n",
    "val_texts = X_val\n",
    "val_labels = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts) #Converting text to a vector of word indexes\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "val_data = pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "trainvalid_labels = to_categorical(np.asarray(train_labels))\n",
    "test_labels = to_categorical(np.asarray(test_labels))\n",
    "val_labels = to_categorical(np.asarray(val_labels))\n",
    "\n",
    "x_train = trainvalid_data\n",
    "y_train = trainvalid_labels\n",
    "x_val = val_data\n",
    "y_val = val_labels\n",
    "\n",
    "#w = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpvPVCwO7bDj"
   },
   "source": [
    "# Keras Embedding Layer\n",
    "\n",
    "A [Keras Embedding Layer](https://keras.io/layers/embeddings/) can be used to train an embedding for each word in your vocabulary. Each word (or sub-word in this case) will be associated with a 16-dimensional vector (or embedding) that will be trained by the model.\n",
    "\n",
    "See [this tutorial](https://www.tensorflow.org/tutorials/text/word_embeddings?hl=en) to learn more about word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only to clean model info\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Fgoq5haqw8Z5"
   },
   "outputs": [],
   "source": [
    "# Create an embedding layer.\n",
    "#embedding_dim = 16\n",
    "embedding = tf.keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM)#encoder.vocab_size, embedding_dim)\n",
    "# Configure the embedding layer as part of a keras model.\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        embedding, # The embedding layer should be the first layer in a model.\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(EMBEDDING_DIM, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(5),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "Fgoq5haqw8Z5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 6.8991 - accuracy: 0.0494 - val_loss: 4.6968 - val_accuracy: 0.0444\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 4.8716 - accuracy: 0.0450 - val_loss: 4.5131 - val_accuracy: 0.0422\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 3.3396 - accuracy: 0.0333 - val_loss: 1.2659 - val_accuracy: 0.0333\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 12.8140 - accuracy: 0.0322 - val_loss: 14.9003 - val_accuracy: 0.0333\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 14.8824 - accuracy: 0.0322 - val_loss: 14.9003 - val_accuracy: 0.0333\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 14.8824 - accuracy: 0.0322 - val_loss: 14.9003 - val_accuracy: 0.0333\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 14.8824 - accuracy: 0.0322 - val_loss: 14.9003 - val_accuracy: 0.0333\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 14.8824 - accuracy: 0.0322 - val_loss: 14.9003 - val_accuracy: 0.0333\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 14.8824 - accuracy: 0.0322 - val_loss: 14.9003 - val_accuracy: 0.0333\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 14.8824 - accuracy: 0.0322 - val_loss: 14.9003 - val_accuracy: 0.0333\n"
     ]
    }
   ],
   "source": [
    "# Compile model.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss= 'categorical_crossentropy',#tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train model for one epoch.\n",
    "history = model.fit(\n",
    "    x_train, y_train, epochs=10, validation_data=(val_data, val_labels), validation_steps=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9HmC29hdMnH"
   },
   "source": [
    "## Saving Data for TensorBoard\n",
    "\n",
    "TensorBoard reads tensors and metadata from the logs of your tensorflow projects. The path to the log directory is specified with `log_dir` below. For this tutorial, we will be using `/logs/amazon-example/`.\n",
    "\n",
    "In order to load the data into Tensorboard, we need to save a training checkpoint to that directory, along with metadata that allows for visualization of a specific layer of interest in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "Pi8_SCYRdn9x"
   },
   "outputs": [],
   "source": [
    "# Set up a logs directory, so Tensorboard knows where to look for files.\n",
    "log_dir='./logs/amazon-example/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=MAX_NB_WORDS) #instantiate a vectorizer\n",
    "\n",
    "X_train_dtm = vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'she did a great job' = [213, 59, 96, 128, 0, 0, ..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'In order to load the data into Tensorboard, \\\n",
    "we need to save a training checkpoint to that directory, \\\n",
    "along with metadata that allows for visualization of a specific layer of \\\n",
    "interest in the model. ' = [..., 123, ..., ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ok', 'absolutely', 'delicious', 'kids', 'not']\n",
      "[594, 2, 210, 438, 582]\n"
     ]
    }
   ],
   "source": [
    "# Show elements:\n",
    "print(list(vect.vocabulary_.keys())[0:5])\n",
    "print(list(vect.vocabulary_.values())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect.vocabulary_.keys()\n",
    "def getList(dict):\n",
    "    list = []\n",
    "    for key in dict.keys():\n",
    "        list.append(key)\n",
    "          \n",
    "    return list\n",
    "\n",
    "encoder = getList(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "Pi8_SCYRdn9x"
   },
   "outputs": [],
   "source": [
    "# Save Labels separately on a line-by-line manner.\n",
    "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
    "    for subwords in encoder:\n",
    "        f.write(\"{}\\n\".format(subwords))\n",
    "    # Fill in the rest of the labels with \"unknown\".\n",
    "    for unknown in range(1, len(encoder)):\n",
    "        f.write(\"unknown #{}\\n\".format(unknown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Pi8_SCYRdn9x"
   },
   "outputs": [],
   "source": [
    "# Save the weights we want to analyze as a variable. Note that the first\n",
    "# value represents any unknown word, which is not in the metadata, here\n",
    "# we will remove this value.\n",
    "weights = tf.Variable(model.layers[0].get_weights()[0][1:])\n",
    "# Create a checkpoint from embedding, the filename and key are the\n",
    "# name of the tensor.\n",
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "# Set up config.\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = 'metadata.tsv'\n",
    "projector.visualize_embeddings(log_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "PtL_KzYMBIzP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 26470), started 0:12:01 ago. (Use '!kill 26470' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-caac8bc7d5118c3a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-caac8bc7d5118c3a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now run tensorboard against on log data we just saved.\n",
    "%tensorboard --logdir ./logs/amazon-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtzW8mr_wmbD"
   },
   "source": [
    "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/embedding_projector.png?raw=1\"/> -->"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tensorboard_projector_plugin.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
